# -*- coding: utf-8 -*-
"""jobcategorizing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hybt1JSBshO35w6uql_jM_GSIpbxN7dD
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import time

def scrape_karkidi_jobs(keyword="data science", pages=1):
    headers = {'User-Agent': 'Mozilla/5.0'}
    base_url = "https://www.karkidi.com/Find-Jobs/{page}/all/India?search={query}"
    jobs_list = []

    for page in range(1, pages + 1):
        url = base_url.format(page=page, query=keyword.replace(' ', '%20'))
        print(f"Scraping page: {page} -> {url}")
        response = requests.get(url, headers=headers)

        if response.status_code != 200:
            print(f"Failed to retrieve page {page}. Status code: {response.status_code}")
            continue

        soup = BeautifulSoup(response.content, "html.parser")
        job_blocks = soup.find_all("div", class_="ads-details")

        for job in job_blocks:
            try:
                title = job.find("h4").get_text(strip=True) if job.find("h4") else ""
                company = job.find("a", href=lambda x: x and "Employer-Profile" in x)
                company_name = company.get_text(strip=True) if company else ""
                location = job.find("p").get_text(strip=True) if job.find("p") else ""
                experience = job.find("p", class_="emp-exp")
                experience_text = experience.get_text(strip=True) if experience else ""
                key_skills_tag = job.find("span", string="Key Skills")
                skills = key_skills_tag.find_next("p").get_text(strip=True) if key_skills_tag else ""
                summary_tag = job.find("span", string="Summary")
                summary = summary_tag.find_next("p").get_text(strip=True) if summary_tag else ""

                jobs_list.append({
                    "Title": title,
                    "Company": company_name,
                    "Location": location,
                    "Experience": experience_text,
                    "Summary": summary,
                    "Skills": skills
                })
            except Exception as e:
                print(f"Error parsing job block: {e}")
                continue

        time.sleep(1)  # Be polite

    return pd.DataFrame(jobs_list)

if __name__ == "__main__":
    df_jobs = scrape_karkidi_jobs(keyword="data science", pages=2)
    df_jobs.to_csv("karkidi_jobs.csv", index=False)
    print(df_jobs.head())

import pandas as pd
import re
from sklearn.feature_extraction.text import TfidfVectorizer
import nltk
from nltk.corpus import stopwords

nltk.download("stopwords")
stop_words = set(stopwords.words("english"))

# Load scraped data
df = pd.read_csv("karkidi_jobs.csv")

# Clean the skills text
def clean_text(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9, ]", "", text)  # remove special characters
    text = re.sub(r"\s+", " ", text)  # remove extra spaces
    tokens = [t.strip() for t in text.split(',') if t.strip()]
    tokens = [t for t in tokens if t not in stop_words]
    return " ".join(tokens)

df["Cleaned_Skills"] = df["Skills"].apply(clean_text)

# Vectorize using TF-IDF
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(df["Cleaned_Skills"])

print("TF-IDF matrix shape:", X.shape)

from sklearn.cluster import KMeans

# You can try different values of k, e.g., 5, 7, 10
k = 5
kmeans = KMeans(n_clusters=k, random_state=42)
df["Cluster"] = kmeans.fit_predict(X)

# Preview clustered jobs
print(df[["Title", "Company", "Skills", "Cluster"]].head())

import joblib

joblib.dump(vectorizer, "tfidf_vectorizer.pkl")
joblib.dump(kmeans, "kmeans_model.pkl")

import pandas as pd
import re
import joblib

# Load saved models
vectorizer = joblib.load("tfidf_vectorizer.pkl")
kmeans = joblib.load("kmeans_model.pkl")

# Define cleaning function (same as before)
def clean_text(text):
    if pd.isna(text):
        return ""
    text = text.lower()
    text = re.sub(r"[^a-zA-Z0-9, ]", "", text)
    text = re.sub(r"\s+", " ", text)
    tokens = [t.strip() for t in text.split(',') if t.strip()]
    return " ".join(tokens)

# Load or create new jobs data
df_new = pd.read_csv("karkidi_jobs.csv")  # Example new job scrape
df_new["Cleaned_Skills"] = df_new["Skills"].apply(clean_text)

# Transform with saved vectorizer and predict with KMeans
X_new = vectorizer.transform(df_new["Cleaned_Skills"])
df_new["Predicted_Cluster"] = kmeans.predict(X_new)

# Preview the predictions
print(df_new[["Title", "Skills", "Predicted_Cluster"]].head())

preferred_cluster = 1
matched_jobs = df_new[df_new["Predicted_Cluster"] == preferred_cluster]

if not matched_jobs.empty:
    print(f"üõéÔ∏è New jobs found in your preferred category (Cluster {preferred_cluster}):\n")
    for _, job in matched_jobs.iterrows():
        print(f"- {job['Title']} | Skills: {job['Skills']}")
else:
    print("üò¥ No new jobs in your preferred category today.")